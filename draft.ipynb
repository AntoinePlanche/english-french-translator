{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(\"train-00000-of-00001.parquet\")\n",
    "START_TOKEN = '<START>'\n",
    "PADDING_TOKEN = ''\n",
    "END_TOKEN = '<END>'\n",
    "english_french_vocabulary = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', \n",
    "                        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "                        ':', '<', '=', '>', '?', '@', \n",
    "                        'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', \n",
    "                        'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', \n",
    "                        'Y', 'Z',\n",
    "                        '[', '\\\\', ']', '^', '_', '`', \n",
    "                        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
    "                        'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x',\n",
    "                        'y', 'z', ';',\n",
    "                        '{', '|', '}', '~', 'à', 'â', 'ä', 'æ',\n",
    "                        'ç', 'é', 'è', 'ê', 'ë', 'î', 'ï', \n",
    "                        'ô', 'ö', 'ù', 'û', 'ü', 'œ', PADDING_TOKEN, END_TOKEN]\n",
    "\n",
    "itos = {k:v for k,v in enumerate(english_french_vocabulary)}\n",
    "stoi = {v:k for k,v in enumerate(english_french_vocabulary)}\n",
    "train = train.iloc[1:200_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>french</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This Regulation shall enter into force on the ...</td>\n",
       "      <td>Le présent règlement entre en vigueur le septi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hello, what's that?</td>\n",
       "      <td>Qu'est-ce que c'est que ça ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>And then I will teach you everything i know.</td>\n",
       "      <td>Et alors, je t'apprendrai tout ce que je sais.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Did you find something?</td>\n",
       "      <td>Par ici !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Article 6</td>\n",
       "      <td>Article 6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "1  This Regulation shall enter into force on the ...   \n",
       "2                                Hello, what's that?   \n",
       "3       And then I will teach you everything i know.   \n",
       "4                            Did you find something?   \n",
       "5                                          Article 6   \n",
       "\n",
       "                                              french  \n",
       "1  Le présent règlement entre en vigueur le septi...  \n",
       "2                       Qu'est-ce que c'est que ça ?  \n",
       "3     Et alors, je t'apprendrai tout ce que je sais.  \n",
       "4                                          Par ici !  \n",
       "5                                          Article 6  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['english'] = train['translation'].apply(lambda x: x.get('en', ''))\n",
    "train['french'] = train['translation'].apply(lambda x: x.get('fr', ''))\n",
    "train = train.drop('translation', axis=1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11596, 11593)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_english = train['english'].str.len().max()\n",
    "max_length_french = train['french'].str.len().max()\n",
    "\n",
    "max_length_english, max_length_french"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'all the competences of the Inspectorate and its right to impose sanctions in case of violation of provisions of Labor Code and other acts produced for its implementation.'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exemple = str(train.at[1292,\"french\"])\n",
    "exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_sentence(sentence, vocabulary):\n",
    "    return all(char in vocabulary for char in sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148493"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train[(train['english'].str.len() <= 200) & (train['french'].str.len() <= 200)]\n",
    "train = train[\n",
    "    train['english'].apply(lambda x: is_valid_sentence(x, english_french_vocabulary)) &\n",
    "    train['french'].apply(lambda x: is_valid_sentence(x, english_french_vocabulary))\n",
    "]\n",
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, english_sentences, french_sentences):\n",
    "        self.english_sentences = english_sentences\n",
    "        self.french_sentences = french_sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.english_sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.english_sentences[idx], self.french_sentences[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148493"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = TextDataset( train['english'].tolist(), train['french'].tolist())\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512 # embedding dimension\n",
    "max_length = 200 # maximum number of words for one translation\n",
    "batch_size = 32 # number of \"sentence\" per batch\n",
    "num_heads = 8 # number of heads during the self attention\n",
    "drop_prob = 0.1 # probability of dropout for a better generalization\n",
    "ffn_hidden = 2048 # expend 512 to 2048 during feed forward step\n",
    "num_layers = 1 # number of sequential encoder\n",
    "fr_vocab_size = len(english_french_vocabulary) # number of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset, batch_size)\n",
    "iterator = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This Regulation shall enter into force on the seventh day following its publication in the Official Journal of the European Union.', \"Hello, what's that?\", 'And then I will teach you everything i know.', 'Did you find something?', 'Article 6', \"Oh, honey, it's not your fault.\", \"I'm onto him now.\", 'Here it is.', \"Steven, why don't you read it?\", \"I'll take care of the kids.\", 'Oh shit!', 'Oh, sorry.', \"I tell you, sometimes I can't take it.\", 'How did you know that?', \"Where's Prue?\", 'But mother, it might have been something important!', \"Irisa's not the coddling type.\", 'ANNEX', 'Are you Talley?', 'I smell.', 'What has Matt said?', 'End of Part One', '- What is it?', \"I'm in trouble.\", 'S (4)', 'I know my job.', 'Bobby.', \"- You know I can't.\", \"OK, that's it.\", 'Ah, gross.', 'Come on', \"That's better.\"), (\"Le présent règlement entre en vigueur le septième jour suivant celui de sa publication au Journal officiel de l'Union européenne.\", \"Qu'est-ce que c'est que ça ?\", \"Et alors, je t'apprendrai tout ce que je sais.\", 'Par ici !', 'Article 6', \"- Tu n'es pas responsable.\", 'Je le tiens.', 'Le voilà.', 'Steven, pourquoi ne le lis-tu pas ?', \"Je m'occuperai des enfants.\", 'Oh putain !', 'Oh, désolé.', \"C'est trop dur, parfois.\", '- Comment le savez-vous ? - Vous venez de me le dire.', 'Où est Prue?', \"C'est peut-être important.\", \"Elle n'est pas du genre câlineuse.\", 'ANNEXE', 'Vous êtes Talley ?', 'Je pue.', \"Qu'est-ce qu'il a dit ?\", 'Fin de la première partie', \"- De la tarte au calcaire et de l'argent fondu chaud.\", '- Parce que tu passes de la drogue.', 'S (6)', 'Je connais mon travail.', 'Bobby.', '- Impossible.', \"Voilà, c'était tout.\", 'Ah, dégueu.', 'Allez', \"C'est mieux.\")]\n",
      "[('My man.', 'Proposed amendments In the light of the above considerations it is proposed to amend : 1 .', \"If you can't beat 'em, join 'em.\", 'Each Party may decide on the publication of the decisions and recommendations of this Stabilisation and Association Committee in its respective official publication.', \"- So how's come... - There's a lot of dogs out...\", 'You... you...', 'Even a private investor would not be happy with a remuneration dependent on the capital being used.', 'I called Harold Larkin and the chairman of the economics department.', 'No.', \"Is this you acting as ship's therapist now?\", 'Ray:', 'He needs to find us.', '- Oh, I know.', \"I could perform it with you if you'd prefer.\", 'Saarland', '- How do you know Brittany?', 'No way is there against the well-doers; and Allah is Forgiving, Merciful.', 'I need a ride.', 'What did you miss?', 'Go, now!', 'Attaboy.', \"What don't I know?\", 'KADI AND AL BARAKAAT INTERNATIONAL FOUNDATION v COUNCIL AND COMMISSION', 'He is also a doctor.', 'Official Journal of the European Communities', '\"Yes, Sister, I did my homework.\" They\\'re just...', 'We need evidence.', \"He'll come after you.\", 'My point is,after Dr. Statler came to work for you, the number of deaths you had attributed to sudden respiratory failure went from nine a year to 20.', 'Rebecca, what is going on?', \"I'm done.\", 'This Decision shall take effect on the date of its adoption.'), ('Mon homme.', 'Modifications proposées Eu égard aux considérations qui précèdent , il est proposé de modifier : 1 .', 'Si tu ne peux pas les battre, joins-toi à eux.', \"Chacune des parties peut décider de la publication, dans son journal officiel respectif, des décisions et recommandations de ce comité de stabilisation et d'association.\", 'Vous, non.', 'Attends ici.', \"Un investisseur privé ne pourrait, lui non plus, se satisfaire d'une rémunération qui dépende de l'utilisation.\", \"J'ai appelé Harold Larkin et président du département économie.\", 'Non.', 'Es-tu la psy du vaisseau, maintenant ?', 'Ray :', 'Je ne peux pas.', '- Euh...', 'Je peux le faire avec toi si tu préfère.', 'Sarre', \"C'est une vieille amie. Brittany?\", 'Pas de reproche contre les bienfaiteurs. Allah est Pardonneur et Miséricordieux.', \"J'ai besoin qu'on m'emmène.\", \"Qu'est-ce qui t'a manqué?\", 'Fonce !', 'Bravo.', \"Qu'est-ce que tu me caches ?\", 'KADI ET AL BARAKAAT INTERNATIONAL FOUNDATION / CONSEIL ET COMMISSION', 'Neurochirurgien. Plutôt joli garçon.', 'Journal officiel des Communautés européennes', '\"Oui, ma soeur, j\\'ai fait mes devoirs.\" Elles sont...', 'Nous avons besoin de preuves.', 'Il va venir après toi.', 'Après que le Dr Statler soit venu travailler pour vous, le nombre de morts attribuées à une brusque insuffisance respiratoire est passé de 9 à 20 en un an.', 'Rébecca, que se passe-t-il ?', \"C'est fini.\", 'La présente décision prend effet le jour de son adoption.')]\n",
      "[('Look, this watch is worth over $ 1100.', 'Have any of your friends ever been arrested?', 'No, Dad, I do not want cocoa.', 'I guess. Yeah.', 'Teacher:', 'Commercial costs [3](EUR/tonne)', 'Those who seek power climb up over dead bodies, like over stairs.', 'Mitch...', 'How dare you?', 'Here, put this on.', 'Postal code', 'Police!', 'Yes. Oh, thanks.', '- I like this ballet.', 'No need.', 'They want to identify the Protestants, put targets on their backs.', '_Copy Contact To...', '- What are you doing?', '- I love you.', \"I don't understand what you mean.\", \"I'm so sorry that I didn't come home when you asked me to.\", 'There you are.', 'Come with me.', 'Field projects', 'This is L.A.', 'Bernard?', 'The troll market? Come on, no one has ever found it.', '- Wait.', 'Friends ...', \"He's beautiful.\", 'Quicksand!', 'Nothing happened.'), ('Cette montre coûte plus de 1100$.', \"L'une quelconque de vos amies s'est-elle jamais fait arrêter ?\", 'Non, papa, je ne veux pas de cacao.', \"- J'en ai peur, oui.\", 'Stop, stop.', 'Frais commerciaux [3]en EUR par tonne', 'Pour arriver au sommet, Ies assoiffés de pouvoir marchent sur les cadavres.', 'Mitch...', 'Comment oses- tu?', 'Tiens, enfile ça.', 'Code postal', 'Police !', 'Merci.', \"J'aime ce ballet.\", 'Pas besoin.', 'Ils veulent identifier les Protestants, mettre des cibles sur leurs dos.', '_Copier le contact vers...', '- Vous faites quoi ?', \"- Je t'aime.\", 'Je ne saisis pas.', \"Je suis désolée de ne pas être rentrée quand tu me l'as demandé.\", 'Te voilà.', 'Vines avec moi.', 'Projets opérationnels', \"C'est Los Angeles.\", 'Bernard ?', \"Non... personne ne l'a jamais trouvé.\", '- Que faites-vous ?', 'Des amis, des vagabonds', '- Il est beau.', 'Du sable mouvant !', '- Rien.')]\n",
      "[('Class dismissed.', 'Wait a minute.', \"Don't.\", '- I thought you did.', \"That's just terrific.\", 'Calm down.', \"You're offering me a job.\", \"The Minutes of yesterday's sitting have been distributed.\", \"- I don't know.\", \"And he's not the only one.\", 'Why at my place?', 'Well, if you say so ...', '2 (including 2a)', \"I'd like them in multiple colors.\", 'You got it.', \"You know we wouldn't have come this far just to arrest two-bit check chiselers.\", \"Don't say a word and step outside.\", 'What the hell, man!', 'No one will ever know.', 'He was never caught.', 'Fuck you.', \"I don't think he is.\", '(may impair fertility),', 'Prove it.', \"You stole some poor woman's dress?\", 'But one thing, this precludes you from representing any other plaintiffs with the same action.', \"You're the expert.\", 'Hi.', '- Fuck.', ']', 'Nothing.', 'Pharmaceuticals.'), ('Le cours est fini.', 'Attendez une minute.', '- Pas la peine.', '- Je pensais que si.', 'Génial.', 'Calme-toi.', \"Tu m'offres un emploi.\", \"Le procès-verbal de la séance d'hier a été distribué.\", '- Je ne sais pas.', \"Et il n'est pas le seul.\", 'Pourquoi chez moi ?', '-Bon, si tu le dis...', '2 (y compris 2a)', 'De plein de couleurs.', \"Tu l'as.\", \"On n'est pas venus d'aussi loin pour arrêter deux escrocs à la petite semaine.\", 'Sors de là.', \"Qu'est-ce qui t'a pris ?\", 'Personne le saura.', \"Il n'a jamais été arrêté.\", 'Crève.', 'Je ne pense pas.', '(peut altérer la fertilité),', 'Prouvez-le.', 'Vous avez volé une robe ?', \"Mais une chose, cela vous interdit de représenter d'autres plaignants pour la même action.\", \"C'est vous l'expert.\", 'Salut.', '- Putain.', ']', 'Rien.', '- La pharmacie.')]\n",
      "[('Well, Belle Starr is a Dallas legend.', 'Three.', 'Sorry.', \"It's alright\", 'Go on! Get out of here!', 'Seriously, take a look.', \"They're armed.\", \"- I don't want to run.\", 'Planeta.', 'I would like to know.', \"Yeah, and this is just like... it's just a little shocking.\", 'In the light of the evidence, the Panel recommends an award of SAR 609,601.', '- Open it.', \"I didn't want to have to do this.\", 'Hang on a second.', 'Now you are.', '- He gives me the creeps.', 'What are you doing down here?', 'Got your back.', 'EMail: drsatapathy@sppu-india.org', '- Challenge accepted.', 'Naturally.', 'TOMORROW', 'of 13 October 2011', 'Girl: Oh, my God!', 'is that why you did it?', 'I got a job offer today.', 'After consulting the Committee of the Regions,', 'The man of the hour.', 'Nice to finally meet you.', '= 5. economic sentiment indicator', 'Article 1'), ('Belle Starr est une véritable légende à Dallas', '- 3. - 3.', 'Pardon.', \"C'est bon.\", 'Allez!', 'Vérifiez !', 'Ils sont armés.', '- Je veux pas me terrer.', 'Planeta.', \"J'aimerais le savoir.\", \"Et c'est juste que... c'est un peu choquant.\", 'ii) Analyse et évaluation', '- Ouvrez-le.', 'Je ne voulais pas en arriver là.', 'Attends une seconde.', 'Maintenant, tu as fini.', 'Il me donne la chair de poule.', \"Qu'est-ce que tu...\", 'Je suis derrière toi.', ': drsatapathy@sppu-india.org', '- Défi relevé.', '- Bien entendu, madame.', 'LA VOITURE DE DEMAIN', 'du 13 octobre 2011', '- Mon Dieu !', \"C'est ça, la raison ?\", \"J'ai eu une offre de poste aujourd'hui.\", 'après consultation du Comité des régions,', \"L'homme du moment.\", 'Ravi de vous rencontrer enfin.', '= 5. climat économique, indicateur', 'Article premier')]\n"
     ]
    }
   ],
   "source": [
    "for batch_num, batch in enumerate(iterator):\n",
    "    print(batch)\n",
    "    if batch_num > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence, stoi, start_token=True, end_token = True):\n",
    "    sentence_word_indicies = [stoi[token] for token in list(sentence)]\n",
    "    if start_token:\n",
    "        sentence_word_indicies.insert(0, stoi[START_TOKEN])\n",
    "    if end_token:\n",
    "        sentence_word_indicies.append(stoi[END_TOKEN])\n",
    "    for _ in range(len(sentence_word_indicies), max_length):\n",
    "        sentence_word_indicies.append(stoi[PADDING_TOKEN])\n",
    "    return torch.tensor(sentence_word_indicies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_length):\n",
    "        super().__init__()\n",
    "        self.max_length = max_length\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self):\n",
    "        even_i = torch.arange(0, self.d_model, 2).float()\n",
    "        denominator = torch.pow(10000, even_i/self.d_model)\n",
    "        position = torch.arange(self.max_length).reshape(self.max_length, 1)\n",
    "        even_PE = torch.sin(position / denominator)\n",
    "        odd_PE = torch.cos(position / denominator)\n",
    "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
    "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
    "        return PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceEmbedding(nn.Module):\n",
    "    \"For a given sentence, create an embedding\"\n",
    "    def __init__(self, max_length, d_model, stoi, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
    "        super().__init__()\n",
    "        self.vocab_size = len(stoi)\n",
    "        print(len(stoi))\n",
    "        print(self.vocab_size)\n",
    "        self.max_length = max_length\n",
    "        self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
    "        self.stoi = stoi\n",
    "        self.position_encoder = PositionalEncoding(d_model, max_length)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.START_TOKEN = START_TOKEN\n",
    "        self.END_TOKEN = END_TOKEN\n",
    "        self.PADDING_TOKEN = PADDING_TOKEN\n",
    "    \n",
    "    def batch_tokenize(self, batch, start_token, end_token):\n",
    "\n",
    "        def tokenize(sentence, start_token, end_token):\n",
    "            sentence_word_indicies = [self.stoi[token] for token in list(sentence)]\n",
    "            if start_token:\n",
    "                sentence_word_indicies.insert(0, self.stoi[self.START_TOKEN])\n",
    "            if end_token:\n",
    "                sentence_word_indicies.append(self.stoi[self.END_TOKEN])\n",
    "            for _ in range(len(sentence_word_indicies), self.max_length):\n",
    "                sentence_word_indicies.append(self.stoi[self.PADDING_TOKEN])\n",
    "            return torch.tensor(sentence_word_indicies)\n",
    "\n",
    "        tokenized = []\n",
    "        for sentence in batch:\n",
    "            token_indices = [self.stoi.get(token, self.stoi[self.PADDING_TOKEN]) for token in list(sentence)]\n",
    "            if start_token:\n",
    "                token_indices.insert(0, self.stoi[self.START_TOKEN])\n",
    "            # Ajouter le token END à la fin si nécessaire\n",
    "            if end_token:\n",
    "                token_indices.append(self.stoi[self.END_TOKEN])\n",
    "            token_indices = (token_indices[:self.max_length] if len(token_indices) > self.max_length \n",
    "                         else token_indices + [self.stoi[self.PADDING_TOKEN]] * (self.max_length - len(token_indices)))\n",
    "            tokenized.append(torch.tensor(token_indices))\n",
    "        tokenized = torch.stack(tokenized)\n",
    "        return tokenized.to(get_device())\n",
    "    \n",
    "    def forward(self, x, start_token, end_token): # sentence\n",
    "        x = self.batch_tokenize(x, start_token, end_token)\n",
    "        x = self.embedding(x)\n",
    "        pos = self.position_encoder().to(get_device())\n",
    "        x = self.dropout(x + pos)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, parameters_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameters_shape = parameters_shape # [d_model]\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(parameters_shape))\n",
    "\n",
    "    def forward(self, inputs): #  batch_size * max_length * d_model\n",
    "        dims = [-(i+1) for i in range(len(self.parameters_shape))]\n",
    "        mean = inputs.mean(dim=dims, keepdim=True)\n",
    "        var = ((inputs - mean)**2).mean(dim=dims, keepdim=True)\n",
    "        std = (var + self.eps).sqrt()\n",
    "        y = (inputs - mean) / std\n",
    "        out = self.gamma * y + self.beta\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        # 3 * d_model to simulate three independant matrix, we can consider these three matrices as concatenate together\n",
    "        self.kv_layer = nn.Linear(d_model, 2 * d_model)\n",
    "        self.q_layer = nn.Linear(d_model, d_model)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, y,  mask=None):\n",
    "        batch_size, sequence_length, d_model = x.size()\n",
    "        kv = self.kv_layer(x)\n",
    "        q = self.q_layer(y)\n",
    "        # We create dimension for the heads to parallelize the process.\n",
    "        # The last dimension contains the matrix q, k and v\n",
    "        kv = kv.reshape(batch_size, sequence_length, self.num_heads, 2 * self.head_dim)\n",
    "        q = q.reshape(batch_size, sequence_length, self.num_heads, self.head_dim)\n",
    "        # We move the head dimension to the second position and the sequence length dimension to the third place.\n",
    "        # This allows us to parallelize the calculations of the dot products K and Q for each word and then for each head.\n",
    "        kv = kv.permute(0, 2, 1, 3)\n",
    "        q = q.permute(0, 2, 1, 3)\n",
    "        # We retrieve independent q, k and v matrices by chuking the qkv matrix on the last dimension\n",
    "        k, v = kv.chunk(2, dim=-1)\n",
    "        attention = (q @ k.transpose(-1, -2)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            attention = attention.permute(1, 0, 2, 3) + mask\n",
    "            attention = attention.permute(1, 0, 2, 3)\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "        values = attention @ v\n",
    "        # Concatenation of all the different head, strictly equivalent to (batch_size, sequence_length, d_model)\n",
    "        values = values.permute(0, 2, 1, 3).reshape(batch_size, sequence_length, d_model)\n",
    "        out = self.linear_layer(values)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        # 3 * d_model to simulate three independant matrix, we can consider these three matrices as concatenate together\n",
    "        self.qkv_layer = nn.Linear(d_model, 3 * d_model)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, sequence_length, _ = x.size()\n",
    "        qkv = self.qkv_layer(x)\n",
    "        # We create dimension for the heads to parallelize the process.\n",
    "        # The last dimension contains the matrix q, k and v\n",
    "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n",
    "        # We move the head dimension to the second position and the sequence length dimension to the third place.\n",
    "        # This allows us to parallelize the calculations of the dot products K and Q for each word and then for each head.\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "        # We retrieve independent q, k and v matrices by chuking the qkv matrix on the last dimension\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        attention = (q @ k.transpose(-1, -2)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            attention = attention.permute(1, 0, 2, 3) + mask\n",
    "            attention = attention.permute(1, 0, 2, 3)\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "        values = attention @ v\n",
    "        # Concatenation of all the different head, strictly equivalent to (batch_size, sequence_length, d_model)\n",
    "        values = values.permute(0, 2, 1, 3).reshape(batch_size, sequence_length, self.num_heads*self.head_dim)\n",
    "        out = self.linear_layer(values)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.ffn = FeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
    "\n",
    "\n",
    "    def forward(self, x, self_attention_mask):\n",
    "        residual_x = x\n",
    "        x= self.attention(x, mask=self_attention_mask) # The encoder has to be able to look at any other word in the sentence\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x + residual_x)\n",
    "        residual_x = x\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.norm2(x + residual_x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialEncoder(nn.Sequential):\n",
    "    def forward(self, *inputs):\n",
    "        x, self_attention_mask  = inputs\n",
    "        for module in self._modules.values():\n",
    "            x = module(x, self_attention_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_length, \n",
    "                 stoi, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
    "        super().__init__()\n",
    "        self.sentence_embedding = SentenceEmbedding(max_length, d_model, stoi, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
    "        self.layers = SequentialEncoder(*(EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n",
    "                                      for _ in range(num_layers)))\n",
    "\n",
    "    def forward(self,x, self_attention_mask, start_token, end_token):\n",
    "        x = self.sentence_embedding(x, start_token, end_token)\n",
    "        x = self.layers(x, self_attention_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.cross_attention = MultiHeadCrossAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.ffn = FeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
    "        self.norm3 = LayerNormalization(parameters_shape=[d_model])\n",
    "\n",
    "\n",
    "    def forward(self, x, y, self_attention_mask, cross_attention_mask):\n",
    "        residual_y = y\n",
    "        y = self.attention(y, mask=self_attention_mask)\n",
    "        y = self.dropout1(y)\n",
    "        y = self.norm1(y + residual_y)\n",
    "        residual_y = y\n",
    "        y = self.cross_attention(x, y, mask=cross_attention_mask)\n",
    "        y = self.dropout2(y)\n",
    "        y = self.norm2(y)\n",
    "        y = self.ffn(y)\n",
    "        y = self.dropout3(y)\n",
    "        y = self.norm3(y + residual_y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialDecoder(nn.Sequential):\n",
    "    def forward(self, *inputs):\n",
    "        x, y, self_attention_mask, cross_attention_mask = inputs\n",
    "        for module in self._modules.values():\n",
    "            y = module(x, y, self_attention_mask, cross_attention_mask)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_length, \n",
    "                    stoi, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
    "        super().__init__()\n",
    "        self.sentence_embedding = SentenceEmbedding(max_length, d_model, stoi, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
    "        self.layers = SequentialDecoder(*(DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n",
    "                                      for _ in range(num_layers)))\n",
    "\n",
    "    def forward(self, x, y , self_attention_mask, cross_attention_mask, start_token, end_token):\n",
    "        y = self.sentence_embedding(y, start_token, end_token)\n",
    "        y = self.layers(x, y, self_attention_mask, cross_attention_mask)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113\n",
      "113\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_length,\n",
    "                  stoi, START_TOKEN, END_TOKEN, PADDING_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3210240"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in encoder.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113\n",
      "113\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_length,\n",
    "                  stoi, START_TOKEN, END_TOKEN, PADDING_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4261888"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in decoder.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_length, fr_vocab_size, stoi, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_length, stoi, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
    "        self.decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_length, stoi, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
    "        self.linear = nn.Linear(d_model, fr_vocab_size)\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    def forward(self, x, y, encoder_self_attention_mask=None, decoder_self_attention_mask=None, decoder_cross_attention_mask=None, \n",
    "                enc_start_token=False, enc_end_token=False, dec_start_token=False, dec_end_token=False):\n",
    "        x = self.encoder(x, encoder_self_attention_mask, start_token=enc_start_token, end_token=enc_end_token)\n",
    "        out = self.decoder(x, y, decoder_self_attention_mask, decoder_cross_attention_mask, start_token=dec_start_token, end_token=dec_end_token)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEG_INFTY = -1e9\n",
    "\n",
    "def create_masks(eng_batch, fr_batch):\n",
    "    num_sentences = len(eng_batch)\n",
    "    look_ahead_mask = torch.full([max_length, max_length] , True)\n",
    "    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n",
    "    encoder_padding_mask = torch.full([num_sentences, max_length, max_length] , False)\n",
    "    decoder_padding_mask_self_attention = torch.full([num_sentences, max_length, max_length] , False)\n",
    "    decoder_padding_mask_cross_attention = torch.full([num_sentences, max_length, max_length] , False)\n",
    "\n",
    "    for idx in range(num_sentences):\n",
    "      eng_sentence_length, fr_sentence_length = len(eng_batch[idx]), len(fr_batch[idx])\n",
    "      eng_chars_to_padding_mask = np.arange(eng_sentence_length + 1, max_length)\n",
    "      fr_chars_to_padding_mask = np.arange(fr_sentence_length + 1, max_length)\n",
    "      encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True\n",
    "      encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True\n",
    "      decoder_padding_mask_self_attention[idx, :, fr_chars_to_padding_mask] = True\n",
    "      decoder_padding_mask_self_attention[idx, fr_chars_to_padding_mask, :] = True\n",
    "      decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True\n",
    "      decoder_padding_mask_cross_attention[idx, fr_chars_to_padding_mask, :] = True\n",
    "\n",
    "    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)\n",
    "    decoder_self_attention_mask =  torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n",
    "    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n",
    "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113\n",
      "113\n",
      "113\n",
      "113\n"
     ]
    }
   ],
   "source": [
    "transformer = Transformer(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_length, fr_vocab_size, \n",
    "                          stoi, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
    "\n",
    "criterian = nn.CrossEntropyLoss(ignore_index=stoi[PADDING_TOKEN],\n",
    "                                reduction='none')\n",
    "for params in transformer.parameters():\n",
    "    if params.dim() > 1:\n",
    "        nn.init.xavier_uniform_(params)\n",
    "optim = torch.optim.Adam(transformer.parameters(), lr=1e-4)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[187], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask \u001b[38;5;241m=\u001b[39m create_masks(eng_batch, fr_batch)\n\u001b[0;32m     13\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 14\u001b[0m fr_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43meng_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mfr_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mencoder_self_attention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdecoder_self_attention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdecoder_cross_attention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menc_start_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menc_end_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdec_start_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdec_end_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m labels \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39msentence_embedding\u001b[38;5;241m.\u001b[39mbatch_tokenize(fr_batch, start_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, end_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterian(\n\u001b[0;32m     25\u001b[0m     fr_predictions\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, fr_vocab_size)\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[0;32m     26\u001b[0m     labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     27\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\etlnapa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[184], line 11\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, y, encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask, enc_start_token, enc_end_token, dec_start_token, dec_end_token)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y, encoder_self_attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, decoder_self_attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, decoder_cross_attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[0;32m     10\u001b[0m             enc_start_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, enc_end_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dec_start_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dec_end_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_self_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menc_start_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menc_end_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(x, y, decoder_self_attention_mask, decoder_cross_attention_mask, start_token\u001b[38;5;241m=\u001b[39mdec_start_token, end_token\u001b[38;5;241m=\u001b[39mdec_end_token)\n\u001b[0;32m     13\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(out)\n",
      "File \u001b[1;32mc:\\Users\\etlnapa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[176], line 10\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, x, self_attention_mask, start_token, end_token)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x, self_attention_mask, start_token, end_token):\n\u001b[1;32m---> 10\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentence_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers(x, self_attention_mask)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\etlnapa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[169], line 45\u001b[0m, in \u001b[0;36mSentenceEmbedding.forward\u001b[1;34m(self, x, start_token, end_token)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, start_token, end_token): \u001b[38;5;66;03m# sentence\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_tokenize(x, start_token, end_token)\n\u001b[1;32m---> 45\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m     pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_encoder()\u001b[38;5;241m.\u001b[39mto(get_device())\n\u001b[0;32m     47\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x \u001b[38;5;241m+\u001b[39m pos)\n",
      "File \u001b[1;32mc:\\Users\\etlnapa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\etlnapa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\etlnapa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "transformer.train()\n",
    "transformer.to(device)\n",
    "total_loss = 0\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    iterator = iter(train_loader)\n",
    "    for batch_num, batch in enumerate(iterator):\n",
    "        transformer.train()\n",
    "        eng_batch, fr_batch = batch\n",
    "        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_batch, fr_batch)\n",
    "        optim.zero_grad()\n",
    "        fr_predictions = transformer(eng_batch,\n",
    "                                     fr_batch,\n",
    "                                     encoder_self_attention_mask.to(device), \n",
    "                                     decoder_self_attention_mask.to(device), \n",
    "                                     decoder_cross_attention_mask.to(device),\n",
    "                                     enc_start_token=False,\n",
    "                                     enc_end_token=False,\n",
    "                                     dec_start_token=True,\n",
    "                                     dec_end_token=True)\n",
    "        labels = transformer.decoder.sentence_embedding.batch_tokenize(fr_batch, start_token=False, end_token=True)\n",
    "        loss = criterian(\n",
    "            fr_predictions.view(-1, fr_vocab_size).to(device),\n",
    "            labels.view(-1).to(device)\n",
    "        ).to(device)\n",
    "        valid_indicies = torch.where(labels.view(-1) == stoi[PADDING_TOKEN], False, True)\n",
    "        loss = loss.sum() / valid_indicies.sum()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        #train_losses.append(loss.item())\n",
    "        if batch_num % 100 == 0:\n",
    "            print(f\"Iteration {batch_num} : {loss.item()}\")\n",
    "            print(f\"English: {eng_batch[0]}\")\n",
    "            print(f\"French Translation: {fr_batch[0]}\")\n",
    "            fr_sentence_predicted = torch.argmax(fr_predictions[0], axis=1)\n",
    "            predicted_sentence = \"\"\n",
    "            for idx in fr_sentence_predicted:\n",
    "              if idx == stoi[END_TOKEN]:\n",
    "                break\n",
    "              predicted_sentence += itos[idx.item()]\n",
    "            print(f\"French Prediction: {predicted_sentence}\")\n",
    "\n",
    "\n",
    "            transformer.eval()\n",
    "            fr_sentence = (\"\",)\n",
    "            eng_sentence = (\"should we go to the mall?\",)\n",
    "            for word_counter in range(max_length):\n",
    "                encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask= create_masks(eng_sentence, fr_sentence)\n",
    "                predictions = transformer(eng_sentence,\n",
    "                                          fr_sentence,\n",
    "                                          encoder_self_attention_mask.to(device), \n",
    "                                          decoder_self_attention_mask.to(device), \n",
    "                                          decoder_cross_attention_mask.to(device),\n",
    "                                          enc_start_token=False,\n",
    "                                          enc_end_token=False,\n",
    "                                          dec_start_token=True,\n",
    "                                          dec_end_token=False)\n",
    "                next_token_prob_distribution = predictions[0][word_counter] # not actual probs\n",
    "                next_token_index = torch.argmax(next_token_prob_distribution).item()\n",
    "                next_token = itos[next_token_index]\n",
    "                fr_sentence = (fr_sentence[0] + next_token, )\n",
    "                if next_token == END_TOKEN:\n",
    "                  break\n",
    "            \n",
    "            print(f\"Evaluation translation (should we go to the mall?) : {fr_sentence}\")\n",
    "            print(\"-------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
