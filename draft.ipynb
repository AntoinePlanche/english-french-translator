{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sqrt(torch.Tensor([4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, parameters_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameters_shape = parameters_shape # [d_model]\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(parameters_shape))\n",
    "\n",
    "    def forward(self, inputs): #  batch_size * max_length * d_model\n",
    "        dims = [-(i+1) for i in range(len(self.parameters_shape))]\n",
    "        print(dims)\n",
    "        mean = inputs.mean(dim=dims, keepdim=True)\n",
    "        var = ((inputs - mean)**2).mean(dim=dims, keepdim=True)\n",
    "        std = (var + self.eps).sqrt()\n",
    "        y = (inputs - mean) / std\n",
    "        out = self.gamma * y + self.beta\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        # 3 * d_model to simulate three independant matrix, we can consider these three matrices as concatenate together\n",
    "        self.qkv_layer = nn.Linear(d_model, 3 * d_model)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, sequence_length, _ = x.size()\n",
    "        qkv = self.qkv_layer(x)\n",
    "        # We create dimension for the heads to parallelize the process.\n",
    "        # The last dimension contains the matrix q, k and v\n",
    "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n",
    "        # We move the head dimension to the second position and the sequence length dimension to the third place.\n",
    "        # This allows us to parallelize the calculations of the dot products K and Q for each word and then for each head.\n",
    "        qkv.permute(0, 2, 1, 3)\n",
    "        # We retrieve independent q, k and v matrices by chuking the qkv matrix on the last dimension\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        attention = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            attention += mask\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "        values = attention @ v\n",
    "        # Concatenation of all the different head, strictly equivalent to (batch_size, sequence_length, self.d_model)\n",
    "        values = values.reshape(batch_size, sequence_length, self.num_heads*self.head_dim)\n",
    "        out = self.linear_layer(values)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.ffn = FeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual_x = x\n",
    "        x= self.attention(x, mask=None) # The encoder has to be able to look at any other word in the sentence\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x + residual_x)\n",
    "        residual_x = x\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.norm2(x + residual_x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(*(EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n",
    "                                      for _ in range(num_layers)))\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512 # embedding dimension\n",
    "max_length = 200 # maximum number of words for one translation\n",
    "batch_size = 32 # number of \"sentence\" per batch\n",
    "num_heads = 8 # number of heads during the self attention\n",
    "drop_prob = 0.1 # probability of dropout for a better generalization\n",
    "ffn_hidden = 2048 # expend 512 to 2048 during feed forward step\n",
    "num_layers = 5 # number of sequential encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15761920"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in encoder.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.8273,  0.5176,  1.3205,  ...,  2.1729, -0.5244,  0.6641],\n",
       "         [ 1.0610,  0.0607,  0.3337,  ...,  0.3078, -0.1650,  0.8738],\n",
       "         [-0.5909, -0.7375,  1.8477,  ...,  1.4211, -2.1396, -0.2343],\n",
       "         ...,\n",
       "         [ 0.4979, -0.5947,  0.1580,  ...,  0.0441, -1.6795,  0.3166],\n",
       "         [-0.1200,  1.1569,  0.5789,  ..., -1.0715, -1.3850, -0.8678],\n",
       "         [-0.2148, -0.6836,  2.9846,  ...,  1.3972,  0.7459,  2.1555]],\n",
       "\n",
       "        [[-0.8392, -0.9191,  1.8405,  ...,  0.0996,  1.0430,  0.1627],\n",
       "         [-1.3963, -2.1196,  0.9659,  ..., -0.2220, -0.3350,  1.1400],\n",
       "         [ 0.4950,  0.7012,  1.4349,  ..., -0.7722, -0.1003, -0.2914],\n",
       "         ...,\n",
       "         [ 0.3337,  0.0944,  0.3303,  ..., -0.2441, -1.3602,  0.1699],\n",
       "         [ 0.8172, -1.8289,  0.7876,  ...,  1.3739, -1.2688,  0.8389],\n",
       "         [ 0.4236,  0.6009,  0.9114,  ...,  0.8605, -0.4383, -0.5911]],\n",
       "\n",
       "        [[ 0.1716,  0.1042,  0.2407,  ..., -0.2389, -0.0581,  0.5588],\n",
       "         [-0.9467, -1.1659,  0.9371,  ...,  0.9080, -0.1764,  0.2267],\n",
       "         [-0.6670,  1.1268,  0.8618,  ...,  0.4399, -1.1182,  0.1156],\n",
       "         ...,\n",
       "         [ 1.2650, -0.8490,  1.2448,  ..., -0.3790,  0.7152,  1.3484],\n",
       "         [ 1.6367,  1.4438,  1.1404,  ..., -1.2693, -0.3406,  0.6660],\n",
       "         [-1.4535,  0.2954,  0.5635,  ...,  0.8993,  0.3468, -0.0192]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.4373,  0.5067,  0.9518,  ...,  0.0111,  0.0346, -0.8877],\n",
       "         [-1.3696, -0.5995,  0.8853,  ...,  0.6779, -0.6700, -1.1259],\n",
       "         [ 1.0970, -0.4921,  3.1699,  ..., -0.9306,  0.0888, -0.6911],\n",
       "         ...,\n",
       "         [ 1.2731,  1.7322,  0.8337,  ..., -0.7542, -1.2534, -0.2066],\n",
       "         [-1.1740, -1.5634,  0.3177,  ...,  3.2850, -1.2575, -0.4236],\n",
       "         [ 0.1447,  0.0599,  2.3144,  ..., -0.0144, -0.2142, -0.5149]],\n",
       "\n",
       "        [[-0.2357,  0.1700,  0.9224,  ...,  2.9916, -0.1492,  0.4378],\n",
       "         [ 0.2697, -0.7254,  1.5173,  ..., -1.5139, -1.0494,  0.6894],\n",
       "         [ 1.6377,  1.5332,  0.5288,  ...,  1.0824, -1.0974,  0.5561],\n",
       "         ...,\n",
       "         [ 1.8331, -0.6987,  1.1229,  ...,  1.8704,  0.1108,  0.3292],\n",
       "         [ 0.3585,  0.1694,  0.7653,  ...,  0.4319,  0.5177,  0.2423],\n",
       "         [ 0.0048,  1.6001,  1.1275,  ...,  0.7363,  0.5722, -0.6322]],\n",
       "\n",
       "        [[ 0.5635, -1.5568, -0.3773,  ..., -0.2835, -0.7065, -1.1140],\n",
       "         [ 0.1682, -1.8836, -1.4004,  ...,  0.8866,  1.3145,  0.6974],\n",
       "         [-0.6222,  0.1846,  1.1549,  ...,  1.3219, -0.7756, -1.0850],\n",
       "         ...,\n",
       "         [ 1.1370,  0.3801,  1.7324,  ...,  2.3516, -0.9223, -1.5242],\n",
       "         [-0.1479,  0.1009, -1.1642,  ...,  0.2195, -0.1437,  1.3598],\n",
       "         [-0.2381, -1.3800,  1.5768,  ...,  0.0804, -0.6108, -0.3551]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((batch_size, max_length, d_model))\n",
    "x = encoder(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
